# Eigendecomposition of a Matrix
In this project we will talk about Eigendecomposition of a Matrix, we will start with a brief definitions and Eigendecomposition's relationship to Machine Learning then we will dive into the Calculation of Eigendecomposition and we will do it two ways. First using numpy then we will do it manually to compare each of these ways and note the results.

2) Eigendecomposition of a Matrix:

   1. Definitions:
      1. Matrix Manipulation:
         - Matrix manipulation involves performing operations such as addition, multiplication, transposition, and inversion on matrices. These are essential in 
           representing and transforming data in machine learning.
           
      2. Eigenvalues and Eigenvectors:
         - For a square matrix ğ´, an eigenvector ğ‘£ and its corresponding eigenvalue ğœ† satisfy:
                                             ğ´ğ‘£ = ğœ†ğ‘£
           This means that multiplying matrix ğ´ by vector ğ‘£ only stretches (or shrinks) ğ‘£ by a factor ğœ†, without changing its direction.
         - Normally when we multiply a matrix by a vector, the vector changes. It gets rotated, flipped or stretched. But somtimes there are special vectors that 
           only get stretched or shrunk without changing direction. So these special vectors are called eigenvectors and the amount they get stretched or shrunk is 
           called their eigenvalue.
           
   2. Relationship to Machine Learning:
      - Matrix operations and Eigendecomposition(eigenvalues and eigenvectors) are core mathematical tools in many machine learning techniques, especially in 
        dimensionality reduction, data transformation, and optimization. We will talk briefly about some of these techniques.
          1. Principal Component Analysis (PCA):
             - PCA is one of the most common uses of eigenvalues and eigenvectors. It transforms the original features into a set of orthogonal components 
               (principal components) where these components are derived from the eigenvectors of the covariance matrix of the dataset. The eigenvalues indicate 
               the amount of variance captured by each principal component.
          2. Singular Value Decomposition (SVD):
             - SVD is a powerful matrix factorization technique used to decompose any ğ‘šÃ—ğ‘› matrix ğ´ into three matrices:
                                              A = ğ‘ˆÎ£ğ‘‰^ğ‘‡
               Where:
               - ğ‘ˆ: left singular vectors (orthogonal)
               - Î£: diagonal matrix of singular values (square roots of eigenvalues)
               - ğ‘‰^ğ‘‡: right singular vectors (orthogonal)
          3. Spectral Clustering:
             - This method uses the eigenvectors of a similarity matrix (like a graph Laplacian) to reduce dimensionality before applying clustering algorithms 
               such as k-means.
          4. Latent Semantic Analysis (LSA):
             - In Natural Language Processing (NLP), LSA uses Singular Value Decomposition (SVD), which involves eigenvalues, to identify patterns in relationships 
               between terms and documents.
          5. Linear Discriminant Analysis (LDA):
             - LDA is a supervised method that finds the linear combination of features that best separates classes. It uses eigenvectors of the scatter matrices 
               to perform the projection.
      
   3. Why it is important:
      - Understand how a matrix transforms space, Reduce computational complexity and Capture the most informative structure of the data. In machine learning, this 
        leads to better generalization, interpretability and efficiency.

   4. Calculation of Eigendecomposition via numpy.linalg.eig():
      - The numpy.linalg.eig() function is used to compute the eigenvalues and right eigenvectors of a square matrix.
      - This function helps you solve the eigenvalue equation:
                                                 ğ´ğ‘£ = ğœ†ğ‘£
        Where:
          ğ´ is a square matrix (the input),
          ğ‘£ is an eigenvector,
          ğœ† is the corresponding eigenvalue.
      - Parameters:
        - A : (..., M, M) array_like
          A square matrix. Can be 2D or batched.
      - Returns:
        - w : (â€¦, M) ndarray
          The eigenvalues of matrix A.
        - v : (â€¦, M, M) ndarray
          The eigenvectors of matrix A. Each column v[:, i] is the eigenvector corresponding to w[i].
      
      - Calculation:
        - NumPy is solving the eigenvalue equation:
                                                 ğ´ğ‘£ = ğœ†ğ‘£
          This can be rearranged to:
                                                (ğ´âˆ’ğœ†I)â‹…ğ‘£=0
          This equation has non-trivial solutions (non-zero vectors ğ‘£) only if:
                                               det(ğ´âˆ’ğœ†I)=0
          This is called the characteristic equation. Solving it gives you the eigenvalues ğœ†. Once those are found, the corresponding eigenvectors ğ‘£ can also be 
          determined.
          
      - NumPy Internally Calls LAPACK(Linear Algebra Package):      ...(1)
        - NumPy doesnâ€™t implement this from scratch. Instead, it uses LAPACK, a powerful library written in Fortran that is used in high-performance scientific 
          computing.
        - For real-valued matrices, NumPy calls LAPACKâ€™s dgeev function.
        - For complex-valued matrices, it uses zgeev.
       
      - BLAS(Basic Linear Algebra Subprograms) for Speed            ...(2)
         - LAPACK itself is built on top of BLAS, which are super-optimized for doing fast matrix operations.
           
      - Because of ...(1) and ..(2):
        - np.linalg.eig() is very fast, even for large matrices
        - It is also numerically stable â€” meaning it's resistant to floating-point errors
       
   5. Comparison of Manual Eigendecomposition Calculation and Numpy's eig():
      - If here we are talking about time, then ofcourse NumPy is much faster then any manual implementation.
      - But the real and challenging question here is,
        Do manually computed eigenvalues/vectors differ from NumPyâ€™s eig()?!! the answer for this question can be consider in to way as follows,
        1. In theory:
          - No, the mathematical results should be the same.Eigenvalues and eigenvectors are unique mathematical solutions to the equation:
                                                      ğ´ğ‘£ = ğœ†ğ‘£
            So whether you solve it by hand (manually) or with NumPy, you are solving the same problem.
        2. But in practice:
          - Yes, minor differences can occur due to:
            1. Numerical Precision:
               - Manual calculations (by hand or in simple code) often use rounded values, where NumPy uses floating-point arithmetic with high precision 
                 (typically float64).
            2. Eigenvector Scaling:
               - Eigenvectors are not unique in magnitude. If ğ‘£ is an eigenvector, then so is 2ğ‘£, or ğ‘£, or any non-zero multiple.
               - So your manual result might give:
                                                 ğ‘£ = [1, 2]  whileÂ NumPyÂ gives  ğ‘£ = [0.477, 0.894]
                 Both are valid.
               - !!! NumPy often returns unit vectors (length = 1).
            3. Order of Results:
               - NumPy's eig() function does not guarantees that the calculated eigenvalues and eigrnvectors are returned in a specific order. So,
                 NumPy may return:
                                                 ğœ†1 = 5  and  ğœ†2 = 2
                 The manual implementation might list:
                                                 ğœ†1 = 2  and  ğœ†2 = 5
               - And thatâ€™s totally fine, order doesn't matter unless you're matching pairs.
              
         
          
